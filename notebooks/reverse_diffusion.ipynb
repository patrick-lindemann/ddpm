{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "from diffusers import UNet2DModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformations\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Lambda(lambda t: (t * 2) - 1),\n",
    "    ]\n",
    ")\n",
    "\n",
    "undo_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Lambda(lambda t: (t + 1) / 2),\n",
    "        transforms.Lambda(lambda t: t.permute(1, 2, 0)),\n",
    "        transforms.Lambda(lambda t: (t * 255.0).numpy().astype(np.uint8)),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class diffusion:\n",
    "    def __init__(self, timesteps, scheduler_type=None):\n",
    "        betas = torch.linspace(0.0001, 0.02, timesteps)\n",
    "\n",
    "        alphas = 1 - betas\n",
    "        sqrt_alphas = torch.sqrt(alphas)\n",
    "        alpha_hats = torch.cumprod(1 - betas, axis=0)\n",
    "        sqrt_alpha_hats = torch.sqrt(alpha_hats)\n",
    "\n",
    "        sqrt_one_minus_alpha_hats = torch.sqrt(1 - alpha_hats)\n",
    "\n",
    "        self.betas = betas\n",
    "        self.alphas = alphas\n",
    "        self.sqrt_alphas = sqrt_alphas\n",
    "        self.alpha_hats = alpha_hats\n",
    "        self.sqrt_alpha_hats = sqrt_alpha_hats\n",
    "        self.sqrt_one_minus_alpha_hats = sqrt_one_minus_alpha_hats\n",
    "\n",
    "    def noise_schedule(self, x_0, t, device=\"cpu\"):\n",
    "        noise = torch.randn(size=x_0.shape)\n",
    "\n",
    "        alpha_hats_t = self.alpha_hats[t]\n",
    "        alpha_hats_t = alpha_hats_t.reshape(shape=(t.shape[0], 1, 1, 1))\n",
    "\n",
    "        sqrt_alpha_hats_t = self.sqrt_alpha_hats[t]\n",
    "        sqrt_alpha_hats_t = sqrt_alpha_hats_t.reshape(shape=(t.shape[0], 1, 1, 1))\n",
    "\n",
    "        sqrt_one_minus_alpha_hats_t = self.sqrt_one_minus_alpha_hats[t]\n",
    "        sqrt_one_minus_alpha_hats_t = sqrt_one_minus_alpha_hats_t.reshape(\n",
    "            shape=(t.shape[0], 1, 1, 1)\n",
    "        )\n",
    "\n",
    "        mean = sqrt_alpha_hats_t * x_0\n",
    "        variance = sqrt_one_minus_alpha_hats_t * noise\n",
    "\n",
    "        x_ts = mean + variance\n",
    "\n",
    "        return x_ts.to(device), noise.to(device)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(self, x_t, t, prediction):\n",
    "        x_t = x_t.cpu()\n",
    "        t = t.cpu()\n",
    "        prediction = prediction.cpu()\n",
    "\n",
    "        betas_t = self.betas[t]\n",
    "        sqrt_alphas_t = self.sqrt_alphas[t]\n",
    "\n",
    "        sqrt_one_minus_alpha_hats_t = self.sqrt_one_minus_alpha_hats[t]\n",
    "\n",
    "        x_t_minus_one = (1 / sqrt_alphas_t) * (\n",
    "            x_t - ((betas_t * prediction) / sqrt_one_minus_alpha_hats_t)\n",
    "        )\n",
    "\n",
    "        if t == 0:\n",
    "            return x_t_minus_one\n",
    "        else:\n",
    "            noise = torch.randn_like(x_t)\n",
    "            return x_t_minus_one + torch.sqrt(betas_t) * noise\n",
    "        \n",
    "    def create_image(self, model, img_size=32, timesteps=300, num_channels=3):\n",
    "        with torch.no_grad():\n",
    "            img = torch.randn((1, num_channels, img_size, img_size))\n",
    "            plt.figure(figsize=(15, 2))\n",
    "            plt.axis(\"off\")\n",
    "            num_images = 10\n",
    "            stepsize = int(timesteps / num_images)\n",
    "\n",
    "            for i in range(0, timesteps)[::-1]:\n",
    "                t = torch.full((1,), i, dtype=torch.long)\n",
    "\n",
    "                img = img.to(device)\n",
    "                t = t.to(device)\n",
    "\n",
    "                prediction = model(img, t).sample\n",
    "\n",
    "                img = self.sample(img, t, prediction)\n",
    "\n",
    "                img = torch.clamp(img, -1.0, 1.0)\n",
    "                if i % stepsize == 0:\n",
    "                    plt.subplot(1, num_images, int(i / stepsize) + 1)\n",
    "                    plt.imshow(undo_transform(img.cpu()[0]))\n",
    "            plt.show()\n",
    "            return img.cpu()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_data = torchvision.datasets.FGVCAircraft(\"../data\", download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = torch.utils.data.DataLoader(image_data, batch_size=16, shuffle=True)\n",
    "images, labels = next(iter(data_loader))\n",
    "\n",
    "plt.imshow(undo_transform(images[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "NO_EPOCHS = 50\n",
    "TIME_STEPS = 500\n",
    "\n",
    "diffuser = diffusion(timesteps=TIME_STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet2DModel(\n",
    "    sample_size=64,  # the target image resolution\n",
    "    in_channels=3,  # the number of input channels, 3 for RGB images\n",
    "    out_channels=3,  # the number of output channels\n",
    "    layers_per_block=2,  # how many ResNet layers to use per UNet block\n",
    "    block_out_channels=(IMG_SIZE, IMG_SIZE, IMG_SIZE*2, IMG_SIZE*2, IMG_SIZE*4, IMG_SIZE*4),  # the number of output channels for each UNet block\n",
    "    down_block_types=(\n",
    "        \"DownBlock2D\",  # a regular ResNet downsampling block\n",
    "        \"DownBlock2D\",\n",
    "        \"DownBlock2D\",\n",
    "        \"DownBlock2D\",\n",
    "        \"AttnDownBlock2D\",  # a ResNet downsampling block with spatial self-attention\n",
    "        \"DownBlock2D\",\n",
    "    ),\n",
    "    up_block_types=(\n",
    "        \"UpBlock2D\",  # a regular ResNet upsampling block\n",
    "        \"AttnUpBlock2D\",  # a ResNet upsampling block with spatial self-attention\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    image_data, batch_size=BATCH_SIZE, shuffle=True\n",
    ")\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "for epoch in range(NO_EPOCHS):\n",
    "    for step, batch in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        t = torch.randint(0, TIME_STEPS, (len(batch[0]),), device=device).long()\n",
    "\n",
    "        img_batch_noisy, noise_batch = diffuser.noise_schedule(\n",
    "            x_0=batch[0], t=t.cpu(), device=device\n",
    "        )\n",
    "\n",
    "        predicted_noise_batch = model(img_batch_noisy, t)\n",
    "\n",
    "        loss = loss_fn(predicted_noise_batch.sample, noise_batch)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if step == 0:\n",
    "            print(f\"Epoch {epoch} | step {step:03d} Loss: {loss.item()} \")\n",
    "            print(f\"Using {torch.cuda.mem_get_info()[0]/1000000000:.2f} GB\")\n",
    "            diffuser.create_image(model, img_size=IMG_SIZE, timesteps=TIME_STEPS)\n",
    "\n",
    "model.save_pretrained(\"model_FGVCAircraft_checkpoint\", from_pt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_new = UNet2DModel().from_pretrained('./model_FGVCAircraft_checkpoint/')\n",
    "model_new = model_new.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = diffuser.create_image(model_new, img_size=IMG_SIZE, timesteps=TIME_STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img.cpu()\n",
    "plt.imshow(undo_transform(img.cpu()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
